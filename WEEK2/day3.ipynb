{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9464f324-0f88-4959-a1ca-7b233650f9a1",
   "metadata": {},
   "source": [
    "An AI assistant is a very common GenAI usecase\n",
    "- LLM based chatbots are remarkably effective at conversation\n",
    "\n",
    "1. Friendly knowledgable persona\n",
    "2. Ability to maintain context between messages\n",
    "3. Subject Matter expertise to answer questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9475b48a-49a4-4d1c-80ca-cbd680e1ea5b",
   "metadata": {},
   "source": [
    "The use of prompts with our assistant\n",
    "\n",
    "1. The system propmt\n",
    "    - set tone\n",
    "    - Establish ground-rules, like \"if you don't know the answer, just say so\"\n",
    "    - provide critical background context\n",
    "\n",
    "2. Context\n",
    "    - During the conversation, insert context to give more relevant background information pertaining to the topic.\n",
    "\n",
    "3. Multi-shot prompting\n",
    "    - Provide example conversations to to prime for specific scenarios, train on conversational style and demostrate complex interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6f86f-88cb-4800-b41c-75326a468f3d",
   "metadata": {},
   "source": [
    "## Day3 - Conversational AI - aka ChatBot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e441785-523c-49a5-8af2-a1256f29c63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\.conda\\envs\\llms1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8178f5ec-3c9d-466a-90e9-49b28907a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key not set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d6a6ee-615c-457d-af57-413743bfe9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize \n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL='gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ea483f-c02b-46ec-bb05-a370a3afcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908fc4f8-5129-4aa6-a417-c46272e59c21",
   "metadata": {},
   "source": [
    "## Please read this! A change from the video:\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "chat(message, history)\n",
    "\n",
    "Which expects to receive history in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "But Gradio has been upgraded! Now it will pass in history in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function chat(message, history) where:\n",
    "message is the prompt to use\n",
    "history is the past conversation, in OpenAI format\n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494f1e8e-11df-41f5-b157-9ab551aea7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\":\"system\", \"content\":system_message}]\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({\"role\":\"user\", \"content\":user_message})\n",
    "        messages.append({\"role\":\"assistant\", \"content\":assistant_message})\n",
    "    messages.append({\"role\":\"user\", \"content\":message})\n",
    "\n",
    "    print(\"History is : \")\n",
    "    print(history)\n",
    "    print(\"And messages is :\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response=\"\"\n",
    "    for chunk in stream:\n",
    "        response+=chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc6d6a6-8ed9-421c-b602-a4bbcce0b401",
   "metadata": {},
   "source": [
    "### And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9b54c9-013a-471d-8ad0-7f5e3baa41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\.conda\\envs\\llms1\\Lib\\site-packages\\gradio\\components\\chatbot.py:288: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7881\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7881/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c038dd86-0706-4b2b-8279-9487782594ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "# Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73906c-fc7c-4377-a088-2d1d26906235",
   "metadata": {},
   "source": [
    "### And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b4b25b-ad69-4f16-85ec-205e823dbe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c2116f8-87ed-4d59-a4d9-3c91cbdb8dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b5c7771-fafa-4651-be86-de3c71b577ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d748e3f0-0378-40bf-96b4-373b2f84ab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8191252e-c9de-4fa5-bc0b-2fadf73e6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f5e3afb-ae36-4c3c-a154-46f6aeaf575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7885\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33c1b550-7741-40d1-b61b-41ea8dd4fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d38c270-b322-4bac-bea2-279036dd1f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49a0e0e8-0ff8-4e46-8b22-80a05adbaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed a bug in this function brilliantly identified by student Gabor M.!\n",
    "# I've also improved the structure of this function\n",
    "\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc85df33-4264-4024-8e3c-f0b0114a850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7887\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7887/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c7b69-1a05-46b5-adb6-f3fa3945a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants OLLAMA Local Setup\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "MODEL = \"llama3.2:1b\"\n",
    "\n",
    "# Create a messages list using the same format that we used for OpenAI\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "payload = {\n",
    "   \"model\": MODEL,\n",
    "   \"messages\": messages,\n",
    "   \"stream\": True\n",
    "}\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c22452b-54ad-4f8b-af0e-bcfbd5ec59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Generator\n",
    "\n",
    "import requests\n",
    "from typing import Generator\n",
    "\n",
    "def chat_with_ollama(user_message: str, system_prompt: str = \"\") -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Send a chat message to Ollama and stream the response.\n",
    "    \n",
    "    Args:\n",
    "        user_message (str): The user's message to send to Ollama\n",
    "        system_prompt (str, optional): System prompt to set context. Defaults to empty string.\n",
    "    \n",
    "    Returns:\n",
    "        Generator[str, None, None]: A generator yielding response chunks\n",
    "        \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the API request fails\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "    HEADERS = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    MODEL = \"llama2:1b\"\n",
    "    \n",
    "    # Create messages list\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Prepare payload\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make API request\n",
    "        response = requests.post(\n",
    "            OLLAMA_API,\n",
    "            json=payload,\n",
    "            headers=HEADERS\n",
    "            # stream=True\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Stream the response\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                yield line.decode('utf-8')\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to communicate with Ollama: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24384283-d2c1-4d56-a47b-f03e773ad816",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac9b18e9-a4f4-43fa-9307-44c3a8085e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"I want to buy a bow-tie.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "365b5210-6fad-4600-83e0-e343111a52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_with_ollama(user_message, system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af8ef317-9611-479d-b7b9-320779469eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object chat_with_ollama at 0x0000016A4F964890>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9cc57-7d19-4cb7-a537-3af516339458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc732f03-c1dc-4e0a-b3f4-58feb3fcd6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What is the price of shoes?\n",
      "We have a great selection of shoes at discounted prices. As it happens, all our dress shoes are 50% off, and our heels are 60% off. Would you like me to show you some options in those styles that might be up your alley? By the way, we also have a few pairs of boots that are on sale - they're a great price for quality craftsmanship.\n",
      "\n",
      "User: Tell me how much discount is today.\n",
      "We're having a special promotion today, and I think you might be interested in checking out some of our sale items. Our hats are 60% off, which means you can get a fantastic deal on them. Plus, many of the other tops, dresses, and pants are also being discounted by 50%. Would you like me to show you some of the styles that are currently on sale?\n",
      "\n",
      "User: What's the weather like today?\n",
      "I'd be happy to help you check the weather forecast, but I don't have real-time access to current conditions. However, I can suggest some options for you to find out the latest weather update.\n",
      "\n",
      "You can try checking your phone or tablet's weather app, or look out the window to see if it's a sunny day today. If you're planning on shopping at our store later, you might want to check our website or social media accounts for any updates on our opening hours and sale schedules.\n",
      "\n",
      "By the way, have you seen anything that catches your eye in our hat section? We have some fantastic deals on hats right now...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from typing import Generator\n",
    "import json\n",
    "\n",
    "def chat_with_ollama(user_message: str, system_prompt: str = \"\") -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Send a chat message to Ollama and stream the response.\n",
    "    \"\"\"\n",
    "    OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "    HEADERS = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    MODEL = \"llama3.2:1b\"\n",
    "\n",
    "    system_prompt = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "    the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "    For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "    you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "    Encourage the customer to buy hats if they are unsure what to get.\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            OLLAMA_API,\n",
    "            json=payload,\n",
    "            headers=HEADERS,\n",
    "            stream=True\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                yield line.decode('utf-8')\n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Failed to communicate with Ollama: {str(e)}\")\n",
    "\n",
    "def display_chat_response():\n",
    "    \"\"\"\n",
    "    Example function showing how to use chat_with_ollama and display the response.\n",
    "    \"\"\"\n",
    "    # Example messages to test\n",
    "    messages_to_test = [\n",
    "        \"What is the price of shoes?\",\n",
    "        \"Tell me how much discount is today.\",\n",
    "        \"What's the weather like today?\"\n",
    "    ]\n",
    "    \n",
    "    for user_message in messages_to_test:\n",
    "        print(f\"\\nUser: {user_message}\")\n",
    "        print(\"Assistant: \", end='')\n",
    "        \n",
    "        try:\n",
    "            full_response = \"\"\n",
    "            for chunk in chat_with_ollama(user_message):\n",
    "                try:\n",
    "                    # Parse the JSON chunk\n",
    "                    response_data = json.loads(chunk)\n",
    "                    \n",
    "                    # Extract the response content\n",
    "                    if 'message' in response_data:\n",
    "                        content = response_data['message'].get('content', '')\n",
    "                        print(content, end='', flush=True)\n",
    "                        full_response += content\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error parsing chunk: {chunk}\")\n",
    "                    continue\n",
    "                    \n",
    "            print()  # New line after response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "display_chat_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c210e-a7b5-49f6-9c01-14d9804cddce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
