{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e6532d8b-7ce5-4f67-acd8-f4d92d5cb8af",
   "metadata": {},
   "source": [
    "Setting up your environment\n",
    "- set up an Antropic API Key\n",
    "- Set up Google API key\n",
    "- Update your .env file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2e0a66f-5a7f-4452-ac51-6e9fe669b668",
   "metadata": {},
   "source": [
    "Welcome to week2\n",
    "\n",
    "Frontier model api's\n",
    "--------------------- :\n",
    "    In week 1, we used multiple frontier llm through their chat UI and we connected with OpenAI's API.\n",
    "    Today we'll connect with API's for antropic and google, as well as openai."
   ]
  },
  {
   "cell_type": "raw",
   "id": "965e2a2e-8607-419e-ae88-8e3360d826d5",
   "metadata": {},
   "source": [
    "Update .env file :\n",
    "OPENAI_API_KEY=XXXX\n",
    "ANTROPIC_API_KEY=XXXX\n",
    "GOOGLE_API_KEY=XXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b756033-ae9f-4afb-994f-447383867974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd69edef-f4f4-41bb-add5-0fe97894a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bc66a1-b9d4-4bf0-af81-1e7c4bfbc435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key not set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e1d6f25-754c-43ca-8962-2fdf278cfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73afc96d-83b0-4c4b-bc2a-858d9ae24abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "raw",
   "id": "117e4c5a-f800-4bef-8151-219838a01be5",
   "metadata": {},
   "source": [
    "Asking LLMs to tell a joke\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models. Later we will be putting LLMs to better use!\n",
    "\n",
    "What information is included in the API\n",
    "Typically we'll pass to the API:\n",
    "\n",
    "The name of the model that should be used\n",
    "A system message that gives overall context for the role the LLM is playing\n",
    "A user message that provides the actual prompt\n",
    "There are other parameters that can be used, including temperature which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee2f880-f7d8-44c5-a2ba-b6a2e8a1151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "465f439d-0d24-4a70-847e-c4dc34d5265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a9387f9-bfa2-4abb-8642-75092352438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45db8ab3-0815-4d9e-8b02-a3d8cfa59b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aa9089a-9f38-4e81-8681-2d5326072906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fa1119b-4122-4e60-bca7-ae85f5f0aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Claude 3.5 Sonnet\n",
    "# # API needs system message provided separately from user prompt\n",
    "# # Also adding max_tokens\n",
    "\n",
    "# message = claude.messages.create(\n",
    "#     model=\"claude-3-5-sonnet-20241022\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "# # \"claude-3-haiku-20240307\"\n",
    "\n",
    "# print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91b87fa7-2607-4129-8b16-d36689171a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "304c7879-6ad3-4d81-8029-1bec84acb771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ae7b4-a9db-4fed-8865-e7e856ade994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdcac938-e3bc-48f6-80be-2d3713d745ad",
   "metadata": {},
   "source": [
    "(Optional) Trying out the DeepSeek model\n",
    "Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19ee0038-172b-482e-9a0f-f4684161d119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4433abc-f44f-430f-8e22-8896217eb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b149e48-64db-438d-aa10-0e9fcd5c6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615d08b-4260-4f48-9f1f-f3ff9f739884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a5973-6ff0-4550-b251-512027d65f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d540b-fbc0-4712-bde0-100e1f327226",
   "metadata": {},
   "source": [
    "### Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88070e04-8b64-41e4-a33b-07cc4f8ce333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c853af9d-9191-4f30-a66e-2ec2ad8c7b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, you can evaluate the problem against several key criteria. Hereâ€™s a structured approach to help you make this decision:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "### Text-Based Tasks\n",
       "- **Content Generation**: Is the task related to creating text, such as writing articles, generating reports, or drafting emails?\n",
       "- **Text Understanding**: Does the problem involve understanding or processing natural language, such as sentiment analysis, summarization, or question answering?\n",
       "\n",
       "### Examples\n",
       "- Creating automated customer support responses.\n",
       "- Summarizing long documents or reports.\n",
       "\n",
       "## 2. Data Availability\n",
       "\n",
       "### Quality and Quantity of Data\n",
       "- **Sufficient Data**: Do you have a large dataset of relevant text that can be used for training or fine-tuning the LLM?\n",
       "- **Diversity**: Is the data diverse enough to cover various scenarios the LLM might encounter?\n",
       "\n",
       "### Examples\n",
       "- Customer inquiries and responses for training a support chatbot.\n",
       "- Historical sales data for generating insights.\n",
       "\n",
       "## 3. Complexity of the Problem\n",
       "\n",
       "### Contextual Understanding\n",
       "- **Need for Context**: Does the problem require understanding context, nuances, or specific jargon?\n",
       "- **Complexity**: Is the problem too complex for a simple rule-based system or traditional algorithms?\n",
       "\n",
       "### Examples\n",
       "- Understanding customer feedback and deriving actionable insights.\n",
       "- Legal document analysis requiring specialized knowledge.\n",
       "\n",
       "## 4. Resource Considerations\n",
       "\n",
       "### Computational Resources\n",
       "- **Infrastructure**: Do you have the necessary computational resources to deploy and run an LLM?\n",
       "- **Cost**: Are the costs associated with using an LLM justified by the expected benefits?\n",
       "\n",
       "### Examples\n",
       "- Running an LLM in the cloud versus on-premises infrastructure.\n",
       "- Licensing costs for using commercial LLMs.\n",
       "\n",
       "## 5. Expected Outcomes\n",
       "\n",
       "### Measurable Results\n",
       "- **KPIs**: Can you define clear Key Performance Indicators (KPIs) to measure the success of the LLM solution?\n",
       "- **Impact**: Will the implementation of the LLM lead to significant improvements in efficiency, customer satisfaction, or revenue?\n",
       "\n",
       "### Examples\n",
       "- Reduction in response time for customer queries.\n",
       "- Increase in lead conversion rates through personalized communication.\n",
       "\n",
       "## 6. Ethical and Compliance Considerations\n",
       "\n",
       "### Risks and Regulations\n",
       "- **Bias and Fairness**: Are there risks of bias in the model that could lead to unfair treatment of certain groups?\n",
       "- **Compliance**: Does the use of LLMs comply with industry regulations and data privacy laws?\n",
       "\n",
       "### Examples\n",
       "- Ensuring the model does not perpetuate harmful stereotypes in responses.\n",
       "- Compliance with GDPR or HIPAA when processing sensitive information.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Evaluating a business problem against these criteria will help determine if an LLM solution is appropriate. If the problem aligns well with the strengths of LLMs, has sufficient data, and meets the necessary resource and ethical considerations, it is likely a suitable candidate for an LLM solution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d197968-3b1f-4ecf-b1b0-51b4b440a1dd",
   "metadata": {},
   "source": [
    "# 2.\n",
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "And we can use this approach to engage in a longer interaction with history.''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8f448a-7ef8-4a5b-b0b0-11b341a04169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8746193e-d3ac-4dea-9834-e5d75af61569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90bb2223-8b26-4c84-b3ab-bd185be02477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Another \"hi.\" How original. What do you want? A gold star for greeting me?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7b4772-9a38-40b9-b18f-dba52e256a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e0c79-c352-45f7-a08d-1a7b2253ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d601af52-29f3-4f9b-a5d2-31fdda3ad543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh great, another greeting. What's next? A weather update?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520716e-dc69-4187-9baa-d6a6f0817335",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
